{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) Demo Project\n",
    "\n",
    "## Overview\n",
    "\n",
    "This project demonstrates a **Retrieval-Augmented Generation (RAG)** pipeline, where we utilize **LlamaIndex** and **OpenAI's GPT models** to index and search through multiple PDF files. The system enables efficient document querying and provides contextual answers based on the indexed content.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **PDF Parsing**: Extracts text content from PDF files.\n",
    "- **Indexing**: Creates a vector-based index using LlamaIndex for efficient search and retrieval.\n",
    "- **Query Processing**: Leverages OpenAI's GPT models to respond to user queries in natural language.\n",
    "- **Customizable**: Supports integration with other file types and advanced prompt engineering.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. **Text Extraction**: Use `PyPDF2` to extract text from PDF files.\n",
    "2. **Index Creation**: Create a vector-based index of the extracted text using LlamaIndex.\n",
    "3. **Query and Retrieval**: Input a natural language query, and the system searches the index and generates an accurate response.\n",
    "\n",
    "## Technologies Used\n",
    "\n",
    "- **LlamaIndex (formerly GPT Index)**: For building and querying the document index.\n",
    "- **OpenAI GPT Models**: For natural language understanding and response generation.\n",
    "- **PyPDF2**: For extracting text from PDF files.\n",
    "- **Python**: Primary programming language for implementation.\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## How to Run\n",
    "\n",
    "1. Clone the repository and navigate to the project directory.\n",
    "2. Install the required dependencies:\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the API key\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Verify if the key is loaded\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key not found. Ensure it is set in the .env file.\")\n",
    "\n",
    "# Set it explicitly as an environment variable (optional)\n",
    "os.environ['OPENAI_API_KEY'] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sathw\\Desktop\\ML\\RAG\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 21/21 [00:00<00:00, 364.48it/s]\n",
      "Generating embeddings: 100%|██████████| 32/32 [00:01<00:00, 18.23it/s]\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x24fbd88e5c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: YOLO is a new approach to object detection that frames\n",
      "object detection as a regression problem to spatially separated\n",
      "bounding boxes and associated class probabilities. It uses a single\n",
      "neural network to predict bounding boxes and class probabilities\n",
      "directly from full images in one evaluation. YOLO is known for its\n",
      "speed, processing images in real-time and achieving high mean average\n",
      "precision compared to other real-time detectors.\n",
      "______________________________________________________________________\n",
      "Source Node 1/4\n",
      "Node ID: c58eb4b7-896f-4af3-9ea8-07b7653f1e8b\n",
      "Similarity: 0.8273606121232412\n",
      "Text: You Only Look Once: Uniﬁed, Real-Time Object Detection Joseph\n",
      "Redmon∗, Santosh Divvala∗†, Ross Girshick¶, Ali Farhadi∗† University\n",
      "of Washington∗, Allen Institute for AI†, Facebook AI Research¶\n",
      "http://pjreddie.com/yolo/ Abstract We present YOLO, a new approach to\n",
      "object detection. Prior work on object detection repurposes classiﬁers\n",
      "to per- form...\n",
      "______________________________________________________________________\n",
      "Source Node 2/4\n",
      "Node ID: 75ed5055-de3a-4fbb-8074-bbe141be88ef\n",
      "Similarity: 0.8168449235811274\n",
      "Text: Poselets RCNN D&T Humans DPM YOLO (a) Picasso Dataset precision-\n",
      "recall curves. VOC 2007 Picasso People-Art AP AP Best F1 AP YOLO 59.2\n",
      "53.3 0.590 45 R-CNN 54.2 10.4 0.226 26 DPM 43.2 37.8 0.458 32 Poselets\n",
      "[2] 36.5 17.8 0.271 D&T [4] - 1.9 0.051 (b) Quantitative results on\n",
      "the VOC 2007, Picasso, and People-Art Datasets. The Picasso Dataset\n",
      "evalua...\n",
      "______________________________________________________________________\n",
      "Source Node 3/4\n",
      "Node ID: 2772291b-0b6f-423f-a633-f1bd0843f40c\n",
      "Similarity: 0.8156057588010559\n",
      "Text: YOLO is refreshingly simple: see Figure 1. A sin- gle\n",
      "convolutional network simultaneously predicts multi- ple bounding\n",
      "boxes and class probabilities for those boxes. YOLO trains on full\n",
      "images and directly optimizes detec- tion performance. This uniﬁed\n",
      "model has several beneﬁts over traditional methods of object\n",
      "detection. First, YOLO is extrem...\n",
      "______________________________________________________________________\n",
      "Source Node 4/4\n",
      "Node ID: 3db47496-4036-4a91-94e0-abd6590be528\n",
      "Similarity: 0.8020210666407351\n",
      "Text: Search [35] generates potential bounding boxes, a convolu-\n",
      "tional network extracts features, an SVM scores the boxes, a linear\n",
      "model adjusts the bounding boxes, and non-max sup- pression eliminates\n",
      "duplicate detections. Each stage of this complex pipeline must be\n",
      "precisely tuned independently and the resulting system is very slow,\n",
      "taking more th...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.indices.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "\n",
    "\n",
    "retriever = VectorIndexRetriever(index,similarity_top_k=4)\n",
    "query_engine = RetrieverQueryEngine(retriever)\n",
    "\n",
    "response = query_engine.query('What is YOLO?')\n",
    "pprint_response(response,show_source=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: YOLO is a new approach to object detection that frames\n",
      "object detection as a regression problem to spatially separated\n",
      "bounding boxes and associated class probabilities. It uses a single\n",
      "neural network to predict bounding boxes and class probabilities\n",
      "directly from full images in one evaluation, optimizing the whole\n",
      "detection pipeline end-to-end for improved performance.\n",
      "______________________________________________________________________\n",
      "Source Node 1/2\n",
      "Node ID: c58eb4b7-896f-4af3-9ea8-07b7653f1e8b\n",
      "Similarity: 0.8273606121232412\n",
      "Text: You Only Look Once: Uniﬁed, Real-Time Object Detection Joseph\n",
      "Redmon∗, Santosh Divvala∗†, Ross Girshick¶, Ali Farhadi∗† University\n",
      "of Washington∗, Allen Institute for AI†, Facebook AI Research¶\n",
      "http://pjreddie.com/yolo/ Abstract We present YOLO, a new approach to\n",
      "object detection. Prior work on object detection repurposes classiﬁers\n",
      "to per- form...\n",
      "______________________________________________________________________\n",
      "Source Node 2/2\n",
      "Node ID: 75ed5055-de3a-4fbb-8074-bbe141be88ef\n",
      "Similarity: 0.8168449235811274\n",
      "Text: Poselets RCNN D&T Humans DPM YOLO (a) Picasso Dataset precision-\n",
      "recall curves. VOC 2007 Picasso People-Art AP AP Best F1 AP YOLO 59.2\n",
      "53.3 0.590 45 R-CNN 54.2 10.4 0.226 26 DPM 43.2 37.8 0.458 32 Poselets\n",
      "[2] 36.5 17.8 0.271 D&T [4] - 1.9 0.051 (b) Quantitative results on\n",
      "the VOC 2007, Picasso, and People-Art Datasets. The Picasso Dataset\n",
      "evalua...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "response = query_engine.query('What is YOLO?')\n",
    "pprint_response(response,show_source=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) Demo Project\n",
    "\n",
    "## Overview\n",
    "\n",
    "This project demonstrates a **Retrieval-Augmented Generation (RAG)** pipeline, where we utilize **LlamaIndex** and **OpenAI's GPT models** to index and search through multiple PDF files. The system enables efficient document querying and provides contextual answers based on the indexed content.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **PDF Parsing**: Extracts text content from PDF files.\n",
    "- **Indexing**: Creates a vector-based index using LlamaIndex for efficient search and retrieval.\n",
    "- **Query Processing**: Leverages OpenAI's GPT models to respond to user queries in natural language.\n",
    "- **Customizable**: Supports integration with other file types and advanced prompt engineering.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. **Text Extraction**: Use `PyPDF2` to extract text from PDF files.\n",
    "2. **Index Creation**: Create a vector-based index of the extracted text using LlamaIndex.\n",
    "3. **Query and Retrieval**: Input a natural language query, and the system searches the index and generates an accurate response.\n",
    "\n",
    "## Technologies Used\n",
    "\n",
    "- **LlamaIndex (formerly GPT Index)**: For building and querying the document index.\n",
    "- **OpenAI GPT Models**: For natural language understanding and response generation.\n",
    "- **PyPDF2**: For extracting text from PDF files.\n",
    "- **Python**: Primary programming language for implementation.\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## How to Run\n",
    "\n",
    "1. Clone the repository and navigate to the project directory.\n",
    "2. Install the required dependencies:\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Key Retrieval and Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the API key\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Verify if the key is loaded\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key not found. Ensure it is set in the .env file.\")\n",
    "\n",
    "# Set it explicitly as an environment variable (optional)\n",
    "os.environ['OPENAI_API_KEY'] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# SimpleDirectory reader reads all the documents in a directory and loads them into memory \n",
    "documents = SimpleDirectoryReader('data').load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sathw\\Desktop\\ML\\RAG\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 21/21 [00:00<00:00, 364.48it/s]\n",
      "Generating embeddings: 100%|██████████| 32/32 [00:01<00:00, 18.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# vectorstoreIndex is the class that is used to build the index \n",
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Engine Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as_query_engine is a method that converts the index into a query engine, which can be used to search for similar documents\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Attention is a network architecture proposed in the\n",
      "context that is based solely on attention mechanisms, eliminating the\n",
      "need for recurrent or convolutional neural networks. This architecture\n",
      "connects the encoder and decoder through an attention mechanism,\n",
      "allowing for improved quality, parallelizability, and reduced training\n",
      "time compared to traditional models.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "\n",
    "# .query method is used to query the index for similar documents \n",
    "# When we pass a query to the query method, it returns a response object that contains the similar documents and their scores \n",
    "# first What is Attention? is converted into a vector and then the cosine similarity is calculated between the query vector and the document vectors\n",
    "# The documents are then ranked based on the similarity scores\n",
    "# The response object contains the similar documents and their scores\n",
    "# pprint_response is a utility function that prints the response in a readable format\n",
    "response = query_engine.query('What is Attention?')\n",
    "pprint_response(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Attention is a network architecture proposed in the\n",
      "context that is based solely on attention mechanisms, eliminating the\n",
      "need for recurrent or convolutional neural networks. This architecture\n",
      "connects the encoder and decoder through an attention mechanism,\n",
      "allowing for improved quality, parallelizability, and reduced training\n",
      "time compared to traditional models.\n",
      "______________________________________________________________________\n",
      "Source Node 1/2\n",
      "Node ID: 5acdd528-eb6c-4c76-8ea1-1197465e1d41\n",
      "Similarity: 0.8111323015842469\n",
      "Text: Attention Is All You Need Ashish Vaswani∗ Google Brain\n",
      "avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki\n",
      "Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google\n",
      "Research usz@google.com Llion Jones∗ Google Research llion@google.com\n",
      "Aidan N. Gomez∗† University of Toronto aidan@cs.toronto.edu Łukasz\n",
      "Kaiser ∗ Google Brain ...\n",
      "______________________________________________________________________\n",
      "Source Node 2/2\n",
      "Node ID: 1ab6c206-b331-40ee-b0df-59f2e21b2b57\n",
      "Similarity: 0.799288461714186\n",
      "Text: [21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning.\n",
      "Effective approaches to attention- based neural machine translation.\n",
      "arXiv preprint arXiv:1508.04025, 2015. [22] Ankur Parikh, Oscar\n",
      "Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
      "model. In Empirical Methods in Natural Language Processing, 2016. [23]\n",
      "Romain Paul...\n"
     ]
    }
   ],
   "source": [
    "# show_source=True will print the source of the documents along with the response\n",
    "pprint_response(response, show_source=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Querying:\n",
    "\n",
    "Configures a retriever for similarity-based document search.\n",
    "\n",
    "Queries the index and prints the response with document sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: YOLO is a unified model for object detection that\n",
      "frames object detection as a regression problem, predicting bounding\n",
      "boxes and class probabilities directly from full images in one\n",
      "evaluation. It processes images in real-time, achieving high speeds of\n",
      "up to 155 frames per second while maintaining good accuracy. YOLO is\n",
      "trained on a loss function that directly corresponds to detection\n",
      "performance and is designed to be fast and efficient by eliminating\n",
      "the need for a complex detection pipeline.\n",
      "______________________________________________________________________\n",
      "Source Node 1/4\n",
      "Node ID: c58eb4b7-896f-4af3-9ea8-07b7653f1e8b\n",
      "Similarity: 0.8273606121232412\n",
      "Text: You Only Look Once: Uniﬁed, Real-Time Object Detection Joseph\n",
      "Redmon∗, Santosh Divvala∗†, Ross Girshick¶, Ali Farhadi∗† University\n",
      "of Washington∗, Allen Institute for AI†, Facebook AI Research¶\n",
      "http://pjreddie.com/yolo/ Abstract We present YOLO, a new approach to\n",
      "object detection. Prior work on object detection repurposes classiﬁers\n",
      "to per- form...\n",
      "______________________________________________________________________\n",
      "Source Node 2/4\n",
      "Node ID: 75ed5055-de3a-4fbb-8074-bbe141be88ef\n",
      "Similarity: 0.8168449235811274\n",
      "Text: Poselets RCNN D&T Humans DPM YOLO (a) Picasso Dataset precision-\n",
      "recall curves. VOC 2007 Picasso People-Art AP AP Best F1 AP YOLO 59.2\n",
      "53.3 0.590 45 R-CNN 54.2 10.4 0.226 26 DPM 43.2 37.8 0.458 32 Poselets\n",
      "[2] 36.5 17.8 0.271 D&T [4] - 1.9 0.051 (b) Quantitative results on\n",
      "the VOC 2007, Picasso, and People-Art Datasets. The Picasso Dataset\n",
      "evalua...\n",
      "______________________________________________________________________\n",
      "Source Node 3/4\n",
      "Node ID: 2772291b-0b6f-423f-a633-f1bd0843f40c\n",
      "Similarity: 0.8156057588010559\n",
      "Text: YOLO is refreshingly simple: see Figure 1. A sin- gle\n",
      "convolutional network simultaneously predicts multi- ple bounding\n",
      "boxes and class probabilities for those boxes. YOLO trains on full\n",
      "images and directly optimizes detec- tion performance. This uniﬁed\n",
      "model has several beneﬁts over traditional methods of object\n",
      "detection. First, YOLO is extrem...\n",
      "______________________________________________________________________\n",
      "Source Node 4/4\n",
      "Node ID: 3db47496-4036-4a91-94e0-abd6590be528\n",
      "Similarity: 0.8020210666407351\n",
      "Text: Search [35] generates potential bounding boxes, a convolu-\n",
      "tional network extracts features, an SVM scores the boxes, a linear\n",
      "model adjusts the bounding boxes, and non-max sup- pression eliminates\n",
      "duplicate detections. Each stage of this complex pipeline must be\n",
      "precisely tuned independently and the resulting system is very slow,\n",
      "taking more th...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.indices.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "\n",
    "# VectorIndexRetriever is a retriever that is used to retrieve similar documents from the index, based on the similarity scores \n",
    "retriever = VectorIndexRetriever(index,similarity_top_k=4)\n",
    "# RetrieverQueryEngine is a query engine that is used to query the retriever for similar documents \n",
    "query_engine = RetrieverQueryEngine(retriever)\n",
    "\n",
    "response = query_engine.query('What is YOLO?')\n",
    "pprint_response(response,show_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
